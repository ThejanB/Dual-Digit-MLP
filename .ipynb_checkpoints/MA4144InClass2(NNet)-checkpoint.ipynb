{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56157fb7-11e7-4151-8a9c-e95c4a844924",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# <center>Feedforward and Backpropagation</center>\n",
    "## <center>Inclass Project 2 - MA4144</center>\n",
    "\n",
    "This project contains 10 tasks/questions to be completed, some require written answers. Open a markdown cell below the respective question that require written answers and provide (type) your answers. Questions that required written answers are given in blue fonts. Almost all written questions are open ended, they do not have a correct or wrong answer. You are free to give your opinions, but please provide related answers within the context.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload this PDF on moodle**.\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c45f68-8516-4bd3-80b3-fa38c9fc01bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Outline of the project\n",
    "\n",
    "The aim of the project is to build a Multi Layer perceptron (MLP) model from scratch for binary classification. That is given an input $x$ output the associated class label $0$ or $1$.\n",
    "\n",
    "In particular, we will classify images of handwritten digits ($0, 1, 2, \\cdots, 9$). For example, given a set of handwritten digit images that only contain two digits (Eg: $1$ and $5$) the model will classify the images based on the written digit.\n",
    "\n",
    "For this we will use the MNIST dataset (collection of $28 \\times 28$ images of handwritten digits) - you can find additional information about MNIST [here](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" width=\"250\">\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a0988-42a0-4e8f-abf5-2bc3ddf18304",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the below cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac8649a-c1fe-4236-8ebd-ce3faa43ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c6025-f0e5-440e-ab25-fb22393f186c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 1: Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a0c01b-d4ce-47c6-ad39-80ea85a59543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset as training and testing, then print out the shapes of the data matrices\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135bf15-9a80-4d14-8fb4-e30c9e53a4cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q1.** In the following cell write code to display $5$ random images in train_X and it's corresponding label in train_y. Each time it is run, you should get a different set of images. The imshow function in the matplotlib library could be useful. Display them as [grayscale images](https://en.wikipedia.org/wiki/Grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad09af0-90d7-4590-922e-d114755455a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO Code to display 5 random handwritten images from train_X and corresponding labels from train_y\n",
    "\n",
    "# Setting a fixed size for the plots\n",
    "plt.figure(figsize=(10, 2))\n",
    "\n",
    "# Selecting 5 random indices from the range of train_X's length\n",
    "indices = random.sample(range(len(train_X)), 5)\n",
    "\n",
    "# Looping through the randomly selected indices to plot each image and label\n",
    "for i, index in enumerate(indices):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(train_X[index], cmap='gray') # Displaying image in grayscale\n",
    "    plt.title('Label: {}'.format(train_y[index]))\n",
    "    plt.axis('off')  # Turning off the axis to make it look cleaner\n",
    "\n",
    "plt.show()  # Displaying the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fd94a-370c-468b-ba0f-01c2506d4810",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2.** Given two digits $d_1$ and $d_2$, both between $0$ and $9$, in the following cell fill in the function body to extract all the samples corresponding to $d_1$ or $d_2$ only, from the dataset $X$ and labels $y$. You can use the labels $y$ to filter the dataset. Assume that the $i$th image $X[i]$ in $X$ is given by $y[i]$. The function should return the extracted samples $X_{extracted}$ and corresponding labels $y_{extracted}$. Avoid using for loops as much as possible, infact you do not need any for loops. numpy.where function should be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd468f8-f28e-405d-aa0a-ac97470615f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digits(X, y, d1, d2):\n",
    "\n",
    "    assert d1 in range(0, 10), \"d1 should be a number between 0 and 9 inclusive\"\n",
    "    assert d2 in range(0, 10), \"d2 should be a number between 0 and 9 inclusive\"\n",
    "    \n",
    "    #TODO\n",
    "\n",
    "    return (X_extracted, y_extracted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29123736-f5a7-4981-afd8-cdb69bae3f7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q3.** Both the training dataset train_X and test_y is a 3 dimensional numpy array, each image occupies 2 dimensions. For convenience of processing data we usually comvert each $28 \\times 28$ image matrix to a vector with $784$ entries. We call this process **vectorize images**.\n",
    "\n",
    "Once we vectorize the images, the vectorized data set would be structured as follows: $i$th row will correspond to a single image and $j$th column will correspond to the $j$th pixel value of each vectorized image. However going along with the convention we discussed in the lecture, the input to the MLP model will require that the columns correspond to individual images. Hence we also require a transpose of the vectorized results.\n",
    "\n",
    "The pixel values in the images will range from $0$ to $255$. Normalize the pixel values between $0$ and $1$, by dividing each pixel value of each image by the maximum pixel value of that image. Simply divide each column of the resulting matrix above by the max of each column. \n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdN_8m9FEqjqAB07obTmB6gNc7S2rSoGBYaA&s\"></center>\n",
    "\n",
    "Given a dataset $X$ of size $N \\times 28 \\times 28$, in the following cell fill in the function to do the following in order;\n",
    "1. Vectorize the dataset resulting in dataset of size $N \\times 784$.\n",
    "2. Transpose the vectorized result.\n",
    "3. Normalize the pixel values of each image.\n",
    "4. Finally return the vectorized, transposed and normalized dataset $X_{transformed}$.\n",
    "\n",
    "Again, avoid for loops, functions such as numpy.reshape, numpy.max etc should be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50027a-5a36-4279-9972-a2bf449bef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_images(X):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return(X_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc428f8-0eec-4eb2-9df2-53cd924c9c9d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q4.** In the following cell write code to;\n",
    "\n",
    "1. Extract images of the digits $d_1 = 1$ and $d_2 = 5$ with their corresponding labels for both the training set (train_X, train_y) and testing set (test_X, test_y) separately.\n",
    "2. Then vectorize the data, tranpose the result and normlize the images.\n",
    "3. Store the results after the final transformations in numpy arrays train_X_1_5, train_y_1_5, test_X_1_5, test_y_1_5\n",
    "4. Our MLP will output only class labels $0$ and $1$ (not $1$ and $5$), so create numpy arrays to store the class labels as follows:\n",
    "   $d_1 = 1$ -> class label = 0 and $d_2 = 5$ -> class label = 1. Store them in the arrays named train_class_1_5, test_class_1_5.\n",
    "\n",
    "Use the above functions you implemented above to complete this task. In addtion, numpy.where could be useful. Avoid for loops as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99382b-43b7-4375-8825-4b7d267b6dac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 2: Implementing MLP from scratch with training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864b18e-cf2e-4da6-a3df-20b8bd84633f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we will implement code to build a customizable MLP model. The hidden layers will have the **Relu activation function** and the final output layer will have **Sigmoid activation function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247c43d-7e37-4e72-9232-5e01898c39a8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q5.** Recall the following about the activation functions:\n",
    "1. Sigmoid activation: $y = \\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "2. Derivative of Sigmoid: $y' = \\sigma'(z) = \\sigma(z) (1 - \\sigma(z)) = y(1-y)$\n",
    "3. ReLu activation: $y = ReLu(z) = max(0, z)$\n",
    "4. Derivative of ReLu: $y' = ReLu'(z) = \\begin{cases} 0 \\; \\textrm{if } z < 0 \\\\ 1 \\; \\textrm{otherwise} \\end{cases} = \\begin{cases} 0 \\; \\textrm{if } y = 0 \\\\ 1 \\; \\textrm{otherwise} \\end{cases}$\n",
    "\n",
    "In the following cell implement the functions to compute activation functions Sigmoid and ReLu given $z$ and derivatives of the Sigmoid and ReLu activation functions given $y$. Note that the input to the derivative functions is $y$ not $z$.\n",
    "\n",
    "In practice the input will not be just single numbers, but matrices. So functions or derivatives should be applied elementwise on matrices. Again avoid for loops, use the power of numpy arrays - search for numpy's capability of doing elementwise computations.\n",
    "\n",
    "Important: When implementing the sigmoid function make sure you handle overflows due to $e^{-z}$ being too large. To avoid you can choose to set the sigmoid value to 'the certain appropriate value' if $z$ is less than a certain good enough negative threshold. If you do not handle overflows, the entire result will be useless fince the MLP will just output Nan (not a number) for evry input at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42dde5-2b7d-4fc2-a5bc-4d396822e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return(sigma)\n",
    "\n",
    "def deriv_sigmoid(Y):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return(sigma_prime)\n",
    "\n",
    "def ReLu(Z):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return(relu)\n",
    "\n",
    "def deriv_ReLu(Y):\n",
    "\n",
    "    #TODO\n",
    "    \n",
    "    return(relu_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca012e-5c82-4689-8170-28f6110cc9b5",
   "metadata": {},
   "source": [
    "**Q6.** The following piece of code defines a simple MLP architecture as a Python class and subsequent initialization of a MLP model. <font color='blue'>Certain lines of code contains commented line numbers. Write a short sentence for each such line explaining its purpose. Feel free to refer to the lecture notes or any resources to answers these question. In addition, explain what the Y, Z, W variables refer to and their purpose</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985eb85-6a89-4d01-b6cf-d58cb8b4d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet:\n",
    "    def __init__(self, input_size = 784, output_size = 1, batch_size = 1000, hidden_layers = [500, 250, 50]):\n",
    "        self.Y = []\n",
    "        self.Z = []\n",
    "        self.W = []\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        layers = [input_size] + hidden_layers + [output_size]\n",
    "        L = len(hidden_layers) + 1\n",
    "    \n",
    "        for i in range(1, L + 1):\n",
    "            self.Y.append(np.zeros((layers[i], batch_size)))                        #line1\n",
    "            self.Z.append(np.zeros((layers[i], batch_size)))                        #Line2\n",
    "            self.W.append(2*(np.random.rand(layers[i], layers[i-1] + 1) - 0.5))     #Line3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854a0e4-9b83-4f52-8b59-3e64af141209",
   "metadata": {},
   "source": [
    "**Answers** (to write answers edit this cell)\n",
    "\n",
    "(i) What does the Y, Z, W variables refer to and their purpose?\n",
    "\n",
    "(ii) Line1: Explanation\n",
    "\n",
    "(iii) Line2: Explanation\n",
    "\n",
    "(iv) Line3: Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee71406-c0f7-4114-9d9c-bf25fe6d208a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q7.** Now we will implement the feedforward algorithm. Recall from the lectures that for each layer $l$ there is input $Y^{(l-1)}$ from the previous layer if $l > 1$ and input data $X$ if $l = 1$. Then we compute $Z^{(l)}$ using the weight matrix $W^{(l)}$ as follows from matrix multiplication:\n",
    "\n",
    "$Z^{(l)} = W^{(l)} Y^{(l-1)}$\n",
    "\n",
    "Make sure that during multiplication you add an additional row of one's to $Y^{(l-1)}$ to accommodate the bias term. However, the rows of ones should not permanently remain on $Y^{(l-1)}$. <font color='blue'>Explain what the bias term is and how adding a row of one's help with the bias terms.</font> During definition above the weight matrices are initialised to afford this extra bias term, so no change to either $Z^{(l)}$ or $W^{(l)}$ is needed.\n",
    "\n",
    "Next compute $Y^{(l)}$, the output of layer $l$ by activation through sigmoid.\n",
    "\n",
    "$Y^{(l)} = \\sigma(Z^{(l)})$\n",
    "\n",
    "The implemented feedforward algorithm should take in a NNet model and an input matrix $X$ and output the modified MLP model - the $Y$'s and $Z$'s computed should be stored in the model for the backpropagation algorithm.\n",
    "\n",
    "As usual avoid for loops as much as possible, use the power of numpy. However, you may use a for loop to iterate through the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a778a-0d06-4295-ac80-3ea6bfbad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(model, X):\n",
    "\n",
    "    #TODO\n",
    "        \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeea889-4e78-4bfc-b126-6d5d11ea571e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Answer** (to write answers edit this cell)\n",
    "\n",
    "Explain what the bias term is and how adding a row of one's help with the bias terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4c037-df9a-49eb-9519-51b86555b33c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q8.** Now we will implement the backpropagation algorithm. The cost function $C$ at the end is given by the square loss.\n",
    "\n",
    "$C = \\frac{1}{2} ||Y^{(L)} - Y||^{2}$, where $Y^{(L)}$ is the final output vector of the feedforward algorithm and $Y$ is the actual label vector associated with the input $X$.\n",
    "\n",
    "At each layer $l = 1, 2, \\cdots, L$ we compute the following (note that the gradients are matrices with the same dimensions as the variable to which we derivating with respect to):\n",
    "\n",
    "1. Gradient of $C$ with respect to $Z^{(l)}$ as <br> $\\frac{\\partial C}{\\partial Z^{(l)}} = deriv(A^{(l)}(Z^{(l)})) \\odot \\frac{\\partial C}{\\partial Y^{(L)}} $, <br> where $A^{(l)}$ is the activation function of the $l$th layer, and we use the derivative of that here. The $\\odot$ refers to the elementwise multiplication.\n",
    "\n",
    "2. Gradient of $C$ with respect to $W^{(l)}$ as <br> $\\frac{\\partial C}{\\partial W^{(l)}} = \\frac{\\partial C}{\\partial Z^{(l)}} (Y^{(l-1)})^{T}$ <br> this is entirely matrix multiplication.\n",
    "\n",
    "3. Gradient of $C$ with respect to $Y^{(l-1)}$ as <br> $\\frac{\\partial C}{\\partial Y^{(l-1)}} = (W^{(l)})^{T} \\frac{\\partial C}{\\partial Z^{(l)}}$ <br> this is also entirely matrix multiplication.\n",
    "\n",
    "4. Update weights by: <br> $W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial C}{\\partial W^{(l)}}$, <br> where $\\eta > 0$ is the learning rate.\n",
    "\n",
    "The loss derivative (the gradient of $C$ with respect to $Y^{(L)}$) at the last layer is given by:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial Y^{(L)}} = Y^{(L)} - Y$\n",
    "\n",
    "By convention we consider $Y^{(0)} = X$, the input data.\n",
    "\n",
    "Based on the backpropagation algorithm implement the backpropagation method in the following cell. Remember to temporarily add a row of ones to $Y^{(l-1)}$ when computing $\\frac{\\partial C}{\\partial W^{(l)}}$ as we discussed back in the feedforward algorithm. Make sure you avoid for loops as much as possible.\n",
    "\n",
    "The function takes in a NNet model, input data $X$ and the corresponding class labels $Y$. learning rate can be set as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958af3f-5a72-462d-844f-aba8ce5b03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, X, Y, eta = 0.01):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fad553-1ba4-4bee-b04b-249b9e04c6a9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q9.** Now implement the training algorithm.\n",
    "\n",
    "The training method takes in training data $X$, actual label $Y$, number of epochs, batch_size, learning rate $\\eta > 0$. The training will happen in epochs. For each epoch, permute the data columns of both $X$ and $Y$, then divide both $X$ and $Y$ into mini batches each with the given batch size. Then run the feedforward and backpropagation for each such batch iteratively.\n",
    "\n",
    "At the end of each iteration, keep trach of the cost $C$ and the $l_2$-norm of change in each weight matrix $W^{(l)}$.\n",
    "\n",
    "At the end of the last epoch, plot the variation cost $C$ and change in weight matrices. Then return the trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4070ea-4a6b-4cba-b882-fa2ea79a9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NNet(X, Y, epochs = 20, batch_size = 1000, eta = 0.01):\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd20299-a129-4d1d-a406-84e1a4dba621",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 3: Evaluation using test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954c37c-f1b7-4e87-aedd-cf9e7624f612",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The following function will evaluate then return an accuracy score and the predicted labels for your model. Do not change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dcbb8-1b5c-4f49-89f6-9bd90833957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_data, test_labels, model, d1, d2):\n",
    "    \n",
    "    L = len(model.hidden_layers) + 1\n",
    "    \n",
    "    Y = test_data\n",
    "    for i in range(L):\n",
    "        Z = np.matmul(model.W[i], np.append(Y, np.array([np.ones(Y.shape[1])]), axis = 0))\n",
    "        if i < L - 1:\n",
    "            Y = ReLu(Z)\n",
    "        else:\n",
    "            Y = sigmoid(Z)\n",
    "    \n",
    "    Y = Y[0]\n",
    "    Y = np.where(Y >= 0.5, 1, 0)\n",
    "    Y_predicted = np.where(Y == 0, d1, d2)\n",
    "\n",
    "    acc = accuracy_score(test_labels, Y_predicted)\n",
    "\n",
    "    return(acc, Y_predicted)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f1db0-8f5f-442c-84f1-578c77b99537",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q10.** Use this test_model function to evaluate your model with the $1$ and $5$ digits. An accuracy $>= 99%$ is achievable. Test with different batch sizes, $\\eta$ values and hidden layers. Find which of those hyperparameters gives the best test accuracy. <font color='blue'>Document the hyperparameter values that gives the best testing accuracy and that best accuracy. Plot a confusion matrix and comment on your observations with reasons. Also, look into the nature of the plots that result fom the training procedure, see how they vary with the hyperparameters and provide your ideas on the observations. Then do the same with a few other pairs of digits $d_1, d_2$. Especially, try $d_1 = 1, d_2 = 7$. Comment on your observations and explain possible reasons</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0561199-92cc-4344-8509-c8ab798c33ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
